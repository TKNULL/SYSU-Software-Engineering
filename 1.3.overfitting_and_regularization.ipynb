{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting & Regularization\n",
    "`jskyzero` `2018/03/04`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 过拟合\n",
    "\n",
    "区合适/过拟合/欠拟合\n",
    "\n",
    "较小的$\\theta_i$\n",
    "+ 简单的假设\n",
    "+ 更小过拟合可能性\n",
    "    \n",
    "将$\\theta_i$加入评估（损耗）函数（被称为正则项），比如：\n",
    "\n",
    "$J(\\theta) = J(\\theta) + \\lambda \\sum_{j=1}^n (\\theta_j^2)$\n",
    "\n",
    "## 梯度下降：\n",
    "\n",
    "repeat until convergence {\n",
    "\n",
    "\n",
    "$\\theta_0 = \\theta_0 - \\alpha \\frac{1}{m} \\sum_{j = 1}^{m}((h_\\theta(x^j) - y^j) * x_0^j)$\n",
    "\n",
    "\n",
    "$\\theta_i = \\theta_i - \\alpha (\\frac{1}{m} \\sum_{j = 1}^{m}((h_\\theta(x^j) - y^j) * x_i^j) + \\frac{\\lambda}{m}\\theta_i)$\n",
    "\n",
    "}\n",
    "\n",
    "$\\theta_i = \\theta_i(1 - \\alpha \\frac{\\lambda}{m}) - \\alpha \\frac{1}{m} \\sum_{j = 1}^{m}((h_\\theta(x^j) - y^j) * x_i^j))$ \n",
    "+ 前半部分会使其变小\n",
    "+ 后半部分一样\n",
    "\n",
    "## 解析解\n",
    "\n",
    "$\\theta = (X^{T}X^{-1})X^T y$\n",
    "\n",
    "$\\theta = (X^{T}X^{-1} + \\lambda A)^{-1} X^T y$\n",
    "\n",
    "```\n",
    "A = 0, 0\n",
    "   0, I\n",
    "\n",
    "eg n = 2\n",
    "A = 0, 0, 0\n",
    "   0, 1, 0\n",
    "   0, 0, 1\n",
    "```\n",
    "\n",
    "if $\\lambda > 0$ 前半部分一定可逆。（正定对称矩阵 $x^T \\space M \\space x > 0 \\space (x \\neq 0)$，正定一定可逆）\n",
    "\n",
    "## 关于参数\n",
    "\n",
    "+ 关于$\\lambda$的讨论\n",
    "    + 如果$\\lambda$过大，会成为损耗函数的主要部分，导致欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 统一公式\n",
    "\n",
    "$J(\\theta) = \\frac {1}{m}(\\lambda R(\\theta) + \\sum_{i=1}^{m}L(h_\\theta(x^i), y^i))$\n",
    "\n",
    "+ Loss function L: 平方误差，Logistic loss, Hinge loss, etc\n",
    "+ Regularization function R: 使预测模型简单/防止过拟合\n",
    "    + L1 norm 可以逼近L0 norm，求解简单\n",
    "    + L2 norm\n",
    "    + Elastic net$\\frac{\\alpha}{2}{||\\theta||}^2 + (1 - \\alpha)||\\theta||_1, \\space 0 < \\alpha < 1$ \n",
    "    \n",
    "## Reference\n",
    "+ Andrew Ng, Coursera: Machine Learning,\n",
    "https://www.coursera.org/learn/machine-learning\n",
    "+ Andrew Ng, \"Feature selection, L 1 vs. L 2 regularization, and\n",
    "rotational invariance.\" ICML, 2004.\n",
    "+ Robert Tibshirani, \"Regression shrinkage and selection via the\n",
    "lasso.\" Journal of the Royal Statistical Society. Series B\n",
    "(Methodological) (1996): 267-288.\n",
    "+ Hui Zou, and Trevor Hastie. \"Regularization and variable\n",
    "selection via the elastic net.\" Journal of the Royal Statistical\n",
    "Society: Series B (Statistical Methodology) 67.2 (2005): 301-\n",
    "320.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
