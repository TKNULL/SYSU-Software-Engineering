{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA, Recommender System, and Association Analysis\n",
    "`jskyzero` `2018/06/03`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主成分分析（Principal Component Analysis，PCA）\n",
    "\n",
    "\n",
    "请从课程网站或[此链接](https://pan.baidu.com/s/1djZy69OmLWqJDpStjGcQaA)下载 Yale 人脸数据集进行降维。通过 MATLAB 命令 `load('yale_face.mat')`读\n",
    "取数据，包含一个4096 × 165矩阵。此矩阵的每一列是由一张64 × 64灰度人脸图像所转成的向\n",
    "量。例如，可以使用 `imshow(reshape(X(:,1),[64 64]),[])`命令显示第一张人脸图像。\n",
    "\n",
    "1. 试使用 MATLAB 中的 svd 函数实现 PCA 算法，并显示均值图像和前五个特征向量所对应的\n",
    "图像；\n",
    "2. 试对协方差矩阵使用 MATLAB 中的 eig 函数计算特征值，显示前五个最大的特征向量所对\n",
    "应的图像，并比较对数据矩阵使用 svd 函数的所得出的特征向量与运算时间；\n",
    "3. 试计算当降维后的维数分别是 10 和 100 时，保留的方差的比例，并分别利用 10 维和 100\n",
    "维坐标恢复原高维空间中的人脸图像，对前三张人脸图像，对比原图和两张恢复的图像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./PCA/output.png)\n",
    "\n",
    "1.实现代码请参考`./PCA/main.m`，下同。均值图像和SVD方法的前五个特征值对应图像如上图所示。\n",
    "\n",
    "2.EIG方法的前五个特征值对应图像如上图所示，运行时间如上图红色框标记。\n",
    "\n",
    "3.保留方差比例如上图红色框标记，对比三张图像，我们可以发现10维的降维效果是不错的，在保存数据量大幅减少的情况下，图片大体轮廓仍然保持，100维的时候已经产生了噪声，甚至图片细节比原图片还多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推荐系统（Recommender System）\n",
    "\n",
    "考虑以下的 8 个用户（A-H）对 7 部电影评级（1 到 5 级）的一个效用矩阵：\n",
    "\n",
    "|      | A    | B    | C    | D    | E    | F    | G    | H    |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| HP1  | 4    | 4    |      |      | 1    | 1    | 5    |      |\n",
    "| HP2  | 5    | 5    |      | 1     |      |      |      |      |\n",
    "| HP3  |      | 4    | 1    |      |      | 1    | 5    | 4    |\n",
    "| TW   | 5    |      | 2    | 5    |      | 1    | 2    |      |\n",
    "| SW1  | 1    |      | 5    | 4    | 5    |      |      | 1    |\n",
    "| SW2  | 1    |      | 5    |      |      | 4    |      |      |\n",
    "| SW3  |      | 1    |      | 5    |      | 5    | 1    |      |\n",
    "\n",
    "电影的名字 HP1、HP2、HP3 分别代表《哈利波特》（Harry Potter）I、II、III，TW 代表《暮光之城》（Twilight），SW1、SW2 和 SW3 分别代表《星球大战》（Star Wars）I、II、III。 \n",
    "\n",
    "1. 试实现协同过滤算法（Collaborative filtering algorithm，课件第 19 页，不需要进行去均值操作），令正则化参数λ = 0.1，特征向量维数n = 4，学习率α = 0.01，分别计算描述电影特 征的7 × 4矩阵X和预测用户评级的8 × 4模型参数矩阵Θ（定义见课件第 23 页，所得结果 保留小数点后 4 位），并计算预测电影评级的7 × 8效用矩阵即XΘ ′（保留小数点后 1 位）； \n",
    "\n",
    "2. 试计算（1）中预测的电影评级与真实评级的平方误差，即∑ ((𝜃 (𝑗) ) 𝑇 𝑥 (𝑖) − 𝑦 (𝑖,𝑗) ) 2 (𝑖,𝑗): 𝑟(𝑖,𝑗)=1 ， 其中𝑟(𝑖,𝑗), 𝑦 (𝑖,𝑗) , 𝜃 (𝑗) , 𝑥 (𝑖)的定义见课件第 7 页；讨论哪两部电影和 HP1 最相似，哪两部电 影和和 SW1 最相似； \n",
    "\n",
    "3. 试使用一个非零常数对协同过滤算法中的变量𝑥 (1) , … , 𝑥 (𝑛𝑚) , 𝜃 (1) , … , 𝜃 (𝑛𝑢)进行初始化（即 更改课件第 19 页 Collaborative filtering algorithm 的第一步为𝑥 (1) = ⋯ = 𝑥 (𝑛𝑚) = 𝜃 (1) = ⋯ = 𝜃 (𝑛𝑢) = 𝑐𝟏，其中𝑐为非零实数，𝟏为所有元素都是 1 的n = 4维列向量；使用（1）中 相同的参数，分别计算描述电影特征的7 × 4矩阵X和预测用户评级的8 × 4模型参数矩阵Θ （所得结果保留小数点后 4 位），并计算预测电影评级的7 × 8效用矩阵即XΘ ′（保留小数 点后1位）和预测的电影评级与真实评级的平方误差，即∑ ((𝜃 (𝑗) ) 𝑇 𝑥 (𝑖) − 𝑦 (𝑖,𝑗) ) 2 (𝑖,𝑗): 𝑟(𝑖,𝑗)=1 ； 与（1）和（2）中的结果比较，讨论此初始化方法的问题。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = '''\n",
    "| HP1  | 4    | 4    |      |      | 1    | 1    | 5    |      |\n",
    "| HP2  | 5    | 5    |      | 1     |      |      |      |      |\n",
    "| HP3  |      | 4    | 1    |      |      | 1    | 5    | 4    |\n",
    "| TW   | 5    |      | 2    | 5    |      | 1    | 2    |      |\n",
    "| SW1  | 1    |      | 5    | 4    | 5    |      |      | 1    |\n",
    "| SW2  | 1    |      | 5    |      |      | 4    |      |      |\n",
    "| SW3  |      | 1    |      | 5    |      | 5    | 1    |      |\n",
    "'''\n",
    "data_str = map(lambda str : str.strip(' '), data_str.replace('\\n', '').split('|'))\n",
    "data = np.delete(np.array(list(data_str))[0:-1].reshape(7, 10), [0,1], 1)\n",
    "\n",
    "def Error(actual, hypothesis):\n",
    "    total_error = 0;\n",
    "    for (x,y), value in np.ndenumerate(actual):\n",
    "        if not value == \"\":\n",
    "            total_error = total_error + (float(value) - hypothesis[x][y]) ** 2\n",
    "    return total_error\n",
    "\n",
    "def Hypothesis(user, theta):\n",
    "    result = np.zeros((theta.shape[0], user.shape[0]))\n",
    "    for (j, i), value in np.ndenumerate(result):\n",
    "        result[j][i] = np.dot(theta[j], user[i])\n",
    "    return result\n",
    "\n",
    "def CF(data, theta, user, lambda_value=0.1, alpha=0.01):\n",
    "    \n",
    "    derivative = lambda theta, user, x : sum([0 if value == \"\" else (np.dot(theta[j], user[i]) - float(value)) * x  for (j,i), value in np.ndenumerate(data)]) \n",
    "  \n",
    "    times = 0\n",
    "    last_error = Error(data, Hypothesis(user, theta))\n",
    "    while times < 100:\n",
    "        for j in range(0, item_size, 1):\n",
    "            for i in range(0, user_size, 1):\n",
    "                user[i] = user[i] - alpha * (derivative(theta, user, theta[j]) + lambda_value * user[i])\n",
    "                theta[j] = theta[j] - alpha * (derivative(theta, user, user[i]) + lambda_value * theta[j])\n",
    "        new_error = Error(data, Hypothesis(user, theta))\n",
    "#         print(new_error)\n",
    "        if abs(new_error - last_error) < 0.01:\n",
    "            break\n",
    "        else:\n",
    "            last_error = new_error\n",
    "    times = times + 1\n",
    "\n",
    "theta_size=4\n",
    "item_size, user_size = data.shape\n",
    "theta = np.random.rand(item_size, theta_size)\n",
    "user = np.random.rand(user_size, theta_size)\n",
    "CF(data, theta, user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = \n",
      " [[0.906  0.8553 0.9727 1.1754]\n",
      " [0.8424 0.7809 0.874  1.071 ]\n",
      " [0.7816 0.774  0.893  1.0216]\n",
      " [0.7893 0.7428 0.8574 1.0182]\n",
      " [0.7462 0.7565 0.877  0.9709]\n",
      " [0.7575 0.7511 0.8837 1.0255]\n",
      " [0.7586 0.7222 0.8826 1.0163]]\n",
      "user = \n",
      " [[0.7626 0.8266 0.9245 1.089 ]\n",
      " [0.8478 0.8369 0.9026 1.0398]\n",
      " [0.8455 0.7697 0.8874 1.0919]\n",
      " [0.796  0.7502 0.9371 1.0946]\n",
      " [0.8249 0.7364 0.8645 1.001 ]\n",
      " [0.8386 0.7428 0.8553 1.0292]\n",
      " [0.7608 0.7602 0.8936 0.9863]\n",
      " [0.7273 0.7191 0.848  1.0076]]\n",
      "hypothesis = \n",
      " [[3.6 3.6 3.6 3.6 3.4 3.4 3.4 3.3]\n",
      " [3.3 3.3 3.3 3.2 3.1 3.1 3.1 3. ]\n",
      " [3.2 3.2 3.2 3.2 3.  3.  3.  2.9]\n",
      " [3.1 3.1 3.1 3.1 3.  3.  2.9 2.9]\n",
      " [3.1 3.1 3.1 3.  2.9 2.9 2.9 2.8]\n",
      " [3.1 3.1 3.1 3.1 3.  3.  2.9 2.9]\n",
      " [3.1 3.1 3.1 3.1 2.9 3.  2.9 2.8]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "print(\"theta = \\n\", theta)\n",
    "print(\"user = \\n\", user)\n",
    "np.set_printoptions(precision=1)\n",
    "hypothesis = Hypothesis(user, theta)\n",
    "print(\"hypothesis = \\n\", hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 误差如下\n",
    "\n",
    "HP1最相似：\n",
    "SW1最相似："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error = \n",
      " 95.31320707825245\n"
     ]
    }
   ],
   "source": [
    "error = Error(data, Hypothesis(user, theta))\n",
    "print(\"error = \\n\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.尝试初始化如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.1\n",
    "theta_size=4\n",
    "item_size, user_size = data.shape\n",
    "theta = np.ones((item_size, theta_size)) * C\n",
    "user = np.ones((user_size, theta_size)) * C\n",
    "CF(data, theta, user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = \n",
      " [[0.9257 0.9257 0.9257 0.9257]\n",
      " [0.8953 0.8953 0.8953 0.8953]\n",
      " [0.8805 0.8805 0.8805 0.8805]\n",
      " [0.8728 0.8728 0.8728 0.8728]\n",
      " [0.8692 0.8692 0.8692 0.8692]\n",
      " [0.8736 0.8736 0.8736 0.8736]\n",
      " [0.876  0.876  0.876  0.876 ]]\n",
      "user = \n",
      " [[0.8654 0.8654 0.8654 0.8654]\n",
      " [0.8706 0.8706 0.8706 0.8706]\n",
      " [0.8796 0.8796 0.8796 0.8796]\n",
      " [0.896  0.896  0.896  0.896 ]\n",
      " [0.9145 0.9145 0.9145 0.9145]\n",
      " [0.9071 0.9071 0.9071 0.9071]\n",
      " [0.8778 0.8778 0.8778 0.8778]\n",
      " [0.8678 0.8678 0.8678 0.8678]]\n",
      "hypothesis = \n",
      " [[3.2 3.2 3.3 3.3 3.4 3.4 3.3 3.2]\n",
      " [3.1 3.1 3.2 3.2 3.3 3.2 3.1 3.1]\n",
      " [3.  3.1 3.1 3.2 3.2 3.2 3.1 3.1]\n",
      " [3.  3.  3.1 3.1 3.2 3.2 3.1 3. ]\n",
      " [3.  3.  3.1 3.1 3.2 3.2 3.1 3. ]\n",
      " [3.  3.  3.1 3.1 3.2 3.2 3.1 3. ]\n",
      " [3.  3.1 3.1 3.1 3.2 3.2 3.1 3. ]]\n",
      "error = \n",
      " 96.36760625513625\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=4)\n",
    "print(\"theta = \\n\", theta)\n",
    "print(\"user = \\n\", user)\n",
    "np.set_printoptions(precision=1)\n",
    "hypothesis = Hypothesis(user, theta)\n",
    "print(\"hypothesis = \\n\", hypothesis)\n",
    "error = Error(data, Hypothesis(user, theta))\n",
    "print(\"error = \\n\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不知道是不是迭代过程有问题，训练出的参数会趋于相同值，对初始化方法的评判没能提供帮助，不过，理论上随机选取初始化值一定程度上有利于避免收敛到局部最优的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 关联规则\n",
    "\n",
    "![hw4_q3](../docs/hw4_q3.jpg)\n",
    "\n",
    "上面六个图（a）到（f）中的每一个图包含 1000 个商品和 10000 个交易的记录。灰色位置表\n",
    "示存在商品交易，而白色表示不存在商品交易。我们使用 Apriori 算法提取频繁项集，并设定\n",
    "频繁项集的最小支持度为 10%，即 minsup=10%（即频繁项集包含在至少 1000 个交易中）。\n",
    "根据上图回答以下问题：\n",
    "\n",
    "1. 哪一个或几个数据集的频繁项集数目最多？哪一个或几个数据集的频繁项集数目最少？\n",
    "2. 哪一个或几个数据集的频繁项集长度最长（即包含最多商品）？\n",
    "3. 哪一个或几个数据集的频繁项集有最高的最大支持度（highest maximum support）？\n",
    "4. 哪一个或几个数据集的频繁项集有最大的支持度范围（例如频繁项集的支持度范围可以从小于 20%变化到大于 70%）？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于图上并没有标尺，所以只用用目测的估计结果\n",
    "1. \n",
    "    + 频繁项集数目最多？ a\n",
    "    + 频繁项集数目最少？ b，f\n",
    "2. 哪一个或几个数据集的频繁项集长度最长（即包含最多商品）？a\n",
    "3. 哪一个或几个数据集的频繁项集有最高的最大支持度（highest maximum support）？b,e\n",
    "4. 哪一个或几个数据集的频繁项集有最大的支持度范围（例如频繁项集的支持度范围可以从小于 20%变化到大于 70%）？f,b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
