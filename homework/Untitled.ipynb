{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\App\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import h5py\n",
    "import gc\n",
    "import shelve\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# set cofig\n",
    "FILE_PATH = \"D:\\\\Downloads\\\\\"\n",
    "SAMPLE_FILE_NAME = \"submission_sample.csv\"\n",
    "TRAIN_FILE_NAME = \"train_data.mat\"\n",
    "TEST_FILE_NAME = \"test_data_raw.mat\"\n",
    "OUT_FILE_NAME = \"out.csv\"\n",
    "SHELVE_FILE_NAME = \"shelve\"\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "with shelve.open(FILE_PATH + SHELVE_FILE_NAME) as db:\n",
    "    X_train= db['X_train']\n",
    "    X_test = db['X_test']\n",
    "    y_train = db['y_train']\n",
    "    y_test = db['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.40311585\n",
      "Iteration 2, loss = 4.34148965\n",
      "Iteration 3, loss = 3.82412638\n",
      "Iteration 4, loss = 3.47235864\n",
      "Iteration 5, loss = 3.19996412\n",
      "Iteration 6, loss = 2.97519824\n",
      "Iteration 7, loss = 2.78220625\n",
      "Iteration 8, loss = 2.61275542\n",
      "Iteration 9, loss = 2.46098874\n",
      "Iteration 10, loss = 2.32358944\n",
      "Iteration 11, loss = 2.19798514\n",
      "Iteration 12, loss = 2.08218376\n",
      "Iteration 13, loss = 1.97461556\n",
      "Iteration 14, loss = 1.87450195\n",
      "Iteration 15, loss = 1.78129239\n",
      "Iteration 16, loss = 1.69358934\n",
      "Iteration 17, loss = 1.61126207\n",
      "Iteration 18, loss = 1.53376652\n",
      "Iteration 19, loss = 1.46030181\n",
      "Iteration 20, loss = 1.39154744\n",
      "Iteration 21, loss = 1.32605022\n",
      "Iteration 22, loss = 1.26437370\n",
      "Iteration 23, loss = 1.20590217\n",
      "Iteration 24, loss = 1.15050084\n",
      "Iteration 25, loss = 1.09790483\n",
      "Iteration 26, loss = 1.04813819\n",
      "Iteration 27, loss = 1.00096314\n",
      "Iteration 28, loss = 0.95611395\n",
      "Iteration 29, loss = 0.91367471\n",
      "Iteration 30, loss = 0.87340388\n",
      "Iteration 31, loss = 0.83506254\n",
      "Iteration 32, loss = 0.79885462\n",
      "Iteration 33, loss = 0.76422935\n",
      "Iteration 34, loss = 0.73156090\n",
      "Iteration 35, loss = 0.70048689\n",
      "Iteration 36, loss = 0.67097610\n",
      "Iteration 37, loss = 0.64294591\n",
      "Iteration 38, loss = 0.61628427\n",
      "Iteration 39, loss = 0.59104810\n",
      "Iteration 40, loss = 0.56692403\n",
      "Iteration 41, loss = 0.54402754\n",
      "Iteration 42, loss = 0.52231480\n",
      "Iteration 43, loss = 0.50165517\n",
      "Iteration 44, loss = 0.48204591\n",
      "Iteration 45, loss = 0.46336171\n",
      "Iteration 46, loss = 0.44561980\n",
      "Iteration 47, loss = 0.42875791\n",
      "Iteration 48, loss = 0.41261922\n",
      "Iteration 49, loss = 0.39742210\n",
      "Iteration 50, loss = 0.38281905\n",
      "Iteration 51, loss = 0.36903535\n",
      "Iteration 52, loss = 0.35579276\n",
      "Iteration 53, loss = 0.34329792\n",
      "Iteration 54, loss = 0.33128185\n",
      "Iteration 55, loss = 0.31993963\n",
      "Iteration 56, loss = 0.30909350\n",
      "Iteration 57, loss = 0.29877802\n",
      "Iteration 58, loss = 0.28887419\n",
      "Iteration 59, loss = 0.27944176\n",
      "Iteration 60, loss = 0.27046537\n",
      "Iteration 61, loss = 0.26191640\n",
      "Iteration 62, loss = 0.25374387\n",
      "Iteration 63, loss = 0.24590289\n",
      "Iteration 64, loss = 0.23843817\n",
      "Iteration 65, loss = 0.23128041\n",
      "Iteration 66, loss = 0.22445886\n",
      "Iteration 67, loss = 0.21793702\n",
      "Iteration 68, loss = 0.21167679\n",
      "Iteration 69, loss = 0.20570946\n",
      "Iteration 70, loss = 0.19999364\n",
      "Iteration 71, loss = 0.19448692\n",
      "Iteration 72, loss = 0.18920652\n",
      "Iteration 73, loss = 0.18422495\n",
      "Iteration 74, loss = 0.17937799\n",
      "Iteration 75, loss = 0.17475428\n",
      "Iteration 76, loss = 0.17029804\n",
      "Iteration 77, loss = 0.16603553\n",
      "Iteration 78, loss = 0.16195739\n",
      "Iteration 79, loss = 0.15798986\n",
      "Iteration 80, loss = 0.15425658\n",
      "Iteration 81, loss = 0.15057863\n",
      "Iteration 82, loss = 0.14706568\n",
      "Iteration 83, loss = 0.14371648\n",
      "Iteration 84, loss = 0.14047299\n",
      "Iteration 85, loss = 0.13734433\n",
      "Iteration 86, loss = 0.13434846\n",
      "Iteration 87, loss = 0.13144931\n",
      "Iteration 88, loss = 0.12865730\n",
      "Iteration 89, loss = 0.12593404\n",
      "Iteration 90, loss = 0.12335921\n",
      "Iteration 91, loss = 0.12084806\n",
      "Iteration 92, loss = 0.11841353\n",
      "Iteration 93, loss = 0.11609017\n",
      "Iteration 94, loss = 0.11379854\n",
      "Iteration 95, loss = 0.11163152\n",
      "Iteration 96, loss = 0.10951222\n",
      "Iteration 97, loss = 0.10746095\n",
      "Iteration 98, loss = 0.10548247\n",
      "Iteration 99, loss = 0.10356507\n",
      "Iteration 100, loss = 0.10170000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\App\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=[1000], learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=100, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# clf = RandomForestClassifier(criterion=\"gini\",\n",
    "#                              max_features=20,\n",
    "# #                              max_depth=4,\n",
    "#                              n_estimators=140,\n",
    "#                              oob_score=True,\n",
    "#                              n_jobs=8,\n",
    "#                              random_state=0)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "# apply same transformation to test data\n",
    "\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "size = [1000]\n",
    "clf = MLPClassifier(hidden_layer_sizes=size,\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "#                     alpha=0.0001,\n",
    "                    max_iter=100,\n",
    "                    verbose=True,\n",
    "#                     tol=0.001\n",
    "                   )\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# from sklearn import svm`\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 0.999724554039874\n"
     ]
    }
   ],
   "source": [
    "print(\"train error {}\".format(clf.score(X_train, y_train)))\n",
    "# print(\"test error {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape = (19850, 6812)\n",
      "submisstion data shape = (19850, 2)\n"
     ]
    }
   ],
   "source": [
    "test = h5py.File(FILE_PATH + TEST_FILE_NAME)['test_feat']\n",
    "test = np.transpose(np.array(test))\n",
    "print(\"test.shape = {}\".format(test.shape))\n",
    "test = scaler.transform(test) \n",
    "Y_test = clf.predict(test)\n",
    "sample_submission =  pd.read_csv(FILE_PATH + SAMPLE_FILE_NAME, sep=',')\n",
    "print(\"submisstion data shape = {}\".format(sample_submission.shape))\n",
    "sample_submission.iloc[:, 1:2] = Y_test\n",
    "sample_submission.to_csv(FILE_PATH + \"0\" + OUT_FILE_NAME, sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
