{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper import read_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "type(read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 9\n",
    "TRAIN_FILE_PATH = \"./asset/training_data.txt\"\n",
    "\n",
    "VOWELS_TABLE = \"\"\"\\\n",
    "|   AA  |   ɑ   |\n",
    "|   AE  |   æ   |\n",
    "|   AH  |   ʌ   |\n",
    "|   AO  |   ɔ   |\n",
    "|   AW  |   aʊ  |\n",
    "|   AY  |   aɪ  |\n",
    "|   EH  |   ɛ   |\n",
    "|   ER  |   ɜːr |\n",
    "|   EY  |   eɪ  |\n",
    "|   IH  |   ɪ   |\n",
    "|   IY  |   i   |\n",
    "|   OW  |   oʊ  |\n",
    "|   OY  |   ɔɪ  |\n",
    "|   UH  |   ʊ   |\n",
    "|   UW  |   u   |\n",
    "\"\"\"\n",
    "\n",
    "CONSONANT_TABLE = \"\"\"\\\n",
    "|     P     |  p  |\n",
    "|     S     |  s  |\n",
    "|     B     |  b  |\n",
    "|     SH    |  ʃ  |\n",
    "|     CH    |  tʃ |\n",
    "|     T     |  t  |\n",
    "|     D     |  d  |\n",
    "|     TH    |  θ  |\n",
    "|     DH    |  ð  |\n",
    "|     V     |  v  |\n",
    "|     F     |  f  |\n",
    "|     W     |  w  |\n",
    "|     G     |  g  |\n",
    "|     Y     |  j  |\n",
    "|     HH    |  h  |\n",
    "|     Z     |  z  |\n",
    "|     JH    |  dʒ |\n",
    "|     ZH    |  ʒ  |\n",
    "|     K     |  k  |\n",
    "|     L     |  l  |\n",
    "|     M     |  m  |\n",
    "|     N     |  n  |\n",
    "|     NG    |  ŋ  |\n",
    "|     R     |  r  |\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_phonemes(phonemes_table):\n",
    "    ans = {}\n",
    "    phonemes_list = list(filter(lambda x: not x in \"\\n\",\n",
    "                              phonemes_table.replace(\" \", \"\").split(\"|\")))\n",
    "    for x in range(0, len(phonemes_list), 2):\n",
    "        ans[phonemes_list[x]] =  phonemes_list[x+1]\n",
    "    return ans\n",
    "\n",
    "train_data = read_data(TRAIN_FILE_PATH)\n",
    "\n",
    "vowels_dict = init_phonemes(VOWELS_TABLE)\n",
    "consonants_dict = init_phonemes(CONSONANT_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contain_phonemes(phoneme, phonemes_dict):\n",
    "    return phoneme in phonemes_dict\n",
    "\n",
    "def count_contain_phonemes_num(phonemes_list, phonemes_dict):\n",
    "    return len(list(filter(lambda phoneme:contain_phonemes(phoneme, phonemes_dict), phonemes_list)))\n",
    "\n",
    "\n",
    "def get_world(line):\n",
    "    return line.split(\":\")[0]\n",
    "\n",
    "def get_phonemes(line):\n",
    "    return line.split(\":\")[1]\n",
    "\n",
    "\n",
    "def has_number(string):\n",
    "    return any(char.isdigit() for char in string)\n",
    "\n",
    "def get_stress_index(vowels_list):\n",
    "    return [i + 1 for i, x in enumerate(vowels_list) if \"1\" in x][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COED</th>\n",
       "      <td>1</td>\n",
       "      <td>0K 1OW0 2EH1 3D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PURVIEW</th>\n",
       "      <td>1</td>\n",
       "      <td>0P 1ER0 2V 3Y 4UW1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HEHIR</th>\n",
       "      <td>1</td>\n",
       "      <td>0HH 1EH0 2HH 3IH1 4R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MUSCLING</th>\n",
       "      <td>1</td>\n",
       "      <td>0M 1AH0 2S 3AH1 4L 5IH2 6NG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONPOISONOUS</th>\n",
       "      <td>2</td>\n",
       "      <td>0N 1AA0 2N 3P 4OY1 5Z 6AH2 7N 8AH2 9S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              type                                   text\n",
       "COED             1                        0K 1OW0 2EH1 3D\n",
       "PURVIEW          1                     0P 1ER0 2V 3Y 4UW1\n",
       "HEHIR            1                   0HH 1EH0 2HH 3IH1 4R\n",
       "MUSCLING         1            0M 1AH0 2S 3AH1 4L 5IH2 6NG\n",
       "NONPOISONOUS     2  0N 1AA0 2N 3P 4OY1 5Z 6AH2 7N 8AH2 9S"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data_1(line):\n",
    "    line = get_phonemes(line)\n",
    "    phonemes_list = line.split(\" \")\n",
    "    vowels_list = list(filter(lambda phoneme: has_number(phoneme), phonemes_list))\n",
    "    #  return [get_stress_index(vowels_list), line.replace(\"0\", \"\").replace(\"1\", \"\").replace(\"2\", \"\")]\n",
    "    for i in range(len(phonemes_list)):\n",
    "        phonemes_list[i] = str(i) + phonemes_list[i].replace(\"0\", \"\").replace(\"1\", \"\").replace(\"2\", \"\") + (str(vowels_list.index(phonemes_list[i])) if phonemes_list[i] in vowels_list else \"\") \n",
    "    return [get_stress_index(vowels_list), \" \".join(phonemes_list)]\n",
    "\n",
    "df = pd.DataFrame([get_data_1(line) for line in train_data], \n",
    "                  index=[get_world(line) for line in train_data],\n",
    "                  columns=[\"type\", \"text\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_freq_of_tokens(sms):\n",
    "    tokens = {}\n",
    "    for token in sms.split(' '):\n",
    "        if token not in tokens:\n",
    "            tokens[token] = 1\n",
    "        else:\n",
    "            tokens[token] += 1\n",
    "    return tokens\n",
    "\n",
    "# print(get_freq_of_tokens(df.iloc[0].text))\n",
    "\n",
    "# features_and_labels = []\n",
    "# for index in range(len(df)):\n",
    "#     features_and_labels.append((get_freq_of_tokens(df.iloc[index].text), df.iloc[index].type))\n",
    "\n",
    "    \n",
    "# encoder = LabelEncoder()\n",
    "# vectorizer = DictVectorizer(dtype=float, sparse=True)\n",
    "# x, y = list(zip(*features_and_labels))\n",
    "\n",
    "# x = vectorizer.fit_transform(x)\n",
    "# y = encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "# arr = x[0].toarray()\n",
    "# for i in range(len(arr[0])):\n",
    "#     if arr[0][i] > 0:\n",
    "#         print('{}:{}'.format(vectorizer.feature_names_[i], arr[0][i]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869525, 0.8693, 0.00022500000000003073\n",
      "0.870125, 0.868, 0.0021250000000000435\n",
      "0.870925, 0.8642, 0.006724999999999981\n",
      "0.870825, 0.8642, 0.006624999999999992\n",
      "0.8699, 0.8692, 0.0007000000000000339\n",
      "0.8694, 0.8651, 0.0042999999999999705\n",
      "0.8695, 0.8686, 0.0009000000000000119\n",
      "0.870975, 0.8671, 0.003875000000000073\n",
      "0.87015, 0.8664, 0.003750000000000031\n",
      "0.870325, 0.8678, 0.0025249999999999995\n",
      "avg:\n",
      "0.8701650000000001, 0.8669899999999998, 0.003175000000000239\n"
     ]
    }
   ],
   "source": [
    "def test_nb(train, test, log=True):\n",
    "    x_train = [ get_freq_of_tokens(text) for text in  train.text ]\n",
    "    y_train = train.type\n",
    "    x_test = [ get_freq_of_tokens(text) for text in  test.text ]\n",
    "    y_test = test.type\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    vectorizer = DictVectorizer(dtype=float, sparse=True)\n",
    "    \n",
    "    x = vectorizer.fit_transform(x_train)\n",
    "    y = encoder.fit_transform(y_train)\n",
    "    \n",
    "    nb = MultinomialNB(alpha=1)\n",
    "    nb.fit(x, y)\n",
    "    train_err = nb.score(x, y)\n",
    "    \n",
    "    x = vectorizer.transform(x_test)\n",
    "    y = encoder.transform(y_test)\n",
    "    test_err = nb.score(x, y)\n",
    "    \n",
    "    if log:\n",
    "        print(\"{}, {}, {}\".format(train_err, test_err, train_err - test_err))\n",
    "    return train_err, test_err\n",
    " \n",
    "def evalute_nb(times=10, log=True):\n",
    "    total_train_err, total_test_err = 0, 0\n",
    "    for i in range(times):\n",
    "        train_err, test_err = test_nb(*train_test_split(df, test_size=0.2, random_state=RANDOM_SEED+i))\n",
    "        total_train_err = total_train_err + train_err\n",
    "        total_test_err  = total_test_err  + test_err\n",
    "    print(\"avg:\")\n",
    "    print(\"{}, {}, {}\".format(total_train_err / times, total_test_err / times, (total_train_err - total_test_err) / times))\n",
    "\n",
    "evalute_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f458e51afcda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# every value >0.0 will be binarized to 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "nb = BernoulliNB(alpha=1, binarize=0.0) # every value >0.0 will be binarized to 1\n",
    "nb.fit(x, y)\n",
    "nb.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(line):\n",
    "    line = get_phonemes(line)\n",
    "    phonemes_list = line.split(\" \")\n",
    "    vowels_list = list(filter(lambda phoneme: has_number(phoneme), phonemes_list))\n",
    "    consonants_list = list(filter(lambda phoneme: not has_number(phoneme), phonemes_list))\n",
    "    # print(count_contain_phonemes_num(constants_list, consonants_dict))\n",
    "    \n",
    "    vowels_vector = list(vowels_dict)\n",
    "    for i in range(len(vowels_vector)):\n",
    "        vowels_vector[i] = 1 if vowels_vector[i] in [vowel[0:2] for vowel in vowels_list] else 0\n",
    "        \n",
    "    consonants_vector = list(consonants_dict)\n",
    "    for i in range(len(consonants_vector)):\n",
    "        consonants_vector[i] = 1 if consonants_vector[i] in consonants_list else 0\n",
    "        \n",
    "    return [get_stress_index(vowels_list), len(vowels_list), len(consonants_list)] + vowels_vector + consonants_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([get_data(line) for line in train_data], \n",
    "                  index=[get_world(line) for line in train_data],\n",
    "                  columns=[\"stress_index\", \"vowels_size\", \"constans_size\"] + list(vowels_dict) + list(consonants_dict))\n",
    "df.head()\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[(df[\"vowels_size\"] == 1) | (df[\"vowels_size\"] == 5)].size\n",
    "\n",
    "# df.groupby(['vowels_size', 'stress_index']).agg(\n",
    "#     {'constans_size':'count'}).groupby(level=0).apply(lambda x : \n",
    "# x * 100 / float(x.sum())).add_suffix('_Count').reset_index().columns[1]\n",
    "\n",
    "# df.groupby(['vowels_size', 'stress_index']).agg(\n",
    "#     {'constans_size':'count'}).groupby(level=0).apply(lambda x :                          \n",
    "#                                                       x * 100 / float(x.sum())).groupby(\"vowels_size\").plot.pie(subplots=True)\n",
    "\n",
    "feature_list = list(df.corr().stress_index.to_frame().sort_values(by='stress_index', ascending=False).index)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_once(train, test, feature_list, depth=5, log=False):\n",
    "    x_train = train[feature_list]\n",
    "    x_test = test[feature_list]\n",
    "    y_train = train.stress_index\n",
    "    y_test = test.stress_index\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion = \"gini\", max_depth=depth, random_state=RANDOM_SEED)\n",
    "    dtree = clf.fit(x_train, y_train)\n",
    "    train_err = dtree.score(x_train, y_train)\n",
    "    test_err = dtree.score(x_test, y_test)\n",
    "    if log:\n",
    "        print(\"{}, {}, {}\".format(train_err, test_err, train_err - test_err))\n",
    "    return train_err, test_err\n",
    "\n",
    "def evalute(feature_list, times=10, depth=5, log=False):\n",
    "    total_train_err, total_test_err = 0, 0\n",
    "    for i in range(times):\n",
    "        train, test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED + i)\n",
    "        train_err, test_err = experiment_once(train, test, feature_list, depth, log)\n",
    "        total_train_err = total_train_err + train_err\n",
    "        total_test_err  = total_test_err  + test_err\n",
    "    print(\"avg:\")\n",
    "    print(\"{}, {}, {}\".format(total_train_err / times, total_test_err / times, (total_train_err - total_test_err) / times))\n",
    "\n",
    "evalute(feature_list[:10], 10, depth=10, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Gaussian) Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[feature_list]\n",
    "y = df.stress_index\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x, y)\n",
    "gnb.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "neigh = KNeighborsClassifier(n_neighbors=3,weights='uniform',p=2)\n",
    "neigh.fit(x_scaled, y)\n",
    "neigh.score(x_scaled, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
